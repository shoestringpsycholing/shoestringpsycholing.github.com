
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Kruschke Chapter 4 - Shoestring Psycholing</title>
  <meta name="author" content="Scott Jackson">

  
  <meta name="description" content="Woohoo! Bayes&#8217; Rule! Or should it be Bayes&#8217;s? Kruschke goes with Bayes&#8217;, so I guess I will, too, but the linguist in me really &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Shoestring Psycholing" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Shoestring Psycholing</a></h1>
  
    <h2>A language science blog</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:shoestringpsycholing.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Kruschke Chapter 4</h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-11-07T14:42:00-05:00" pubdate data-updated="true">Nov 7<span>th</span>, 2012</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>Woohoo! Bayes&#8217; Rule! Or should it be Bayes&#8217;s?  Kruschke goes with Bayes&#8217;, so I guess I will, too, but the linguist in me really things it ought to be Bayes&#8217;s, or at least pronounced that way.  I&#8217;d certainly say Jonas&#8217;s rule, if I knew a guy name Jonas with a rule.  Ok, sorry, back to Bayes and Kruschke.</p>

<p>I really like the initial examples he starts off with.  For whatever reason, equations are a lot of hard work for me, even though I like them, and it&#8217;s hard for me to understand them deeply enough to have an intuitive feel for what they are saying.  The rain/clouds example is way more accessible, and the playing card probabilities are a nice enough &#8220;toy&#8221; example that actual numbers can be calculated, so overall I think he&#8217;s done a great job in choosing examples.</p>

<p>The gist of Bayes&#8217; Rule is that it sets up a relationship between conditional probabilities, allowing you to calculate something you want to know, from quantities that you already have (or can estimate).  Back to the idea of beliefs as probabilities, Bayesian inference boils down to the idea of calculating beliefs (probability of some parameter) given data, which is a conditional probability, like calculating the probability of rain (a parameter) given clouds (data).  And the point and magic of Bayes&#8217; Rule is that this can be calculated as a function of other probabilities, which we can get easier access to directly.</p>

<p>Of course, &#8220;easier&#8221; is relative, and that&#8217;s where all the computational stuff starts coming into play.  But in this chapter, Kruschke sticks to the coin-flipping and playing-card examples, because the probabilities involved there can be calculated pretty easily.</p>

<p>But again <a href="http://shoestringpsycholing.github.com/blog/2012/11/05/kruschke-chapter-3/">like in the previous chapter</a>, I feel like sometimes I needed to suspend my thoughts for a little to follow how Kruschke lays out things, because it took me a while to get the connection between what he starts off talking about as examples, and things that I see as valuable.  I&#8217;m coming to this book from a fair amount of experience with a bunch of NHST tools, even &#8220;fancy&#8221; ones like mixed-effects models, and there have been times in these initial chapters where I was having trouble seeing the relevance to what I wanted to do, which was try to understand how Bayesian stats are an alternative to the NHST way of doing things.  When he finally gets to talking about belief in a model, or belief in a model parameter, I could start recognizing those things as goals, but it was hard to follow the lead-up.  So again &#8212; and I&#8217;m starting to sound like a broken record &#8212; depending on the audience, a bit of a back-and-forth re-reading of some of these initial chapters will probably be beneficial, in order to connect his very nice, clear description of concepts to the more complex things that you will inevitably care more about. To put it a little more negatively, at this point in the book, if you&#8217;re coming in with any expertise in other statistical methods, you may feel impatient, and it may be hard to work through these chapters without a clear view of how they connect to what you actually want to do. My two cents is that it&#8217;s worth the effort to suffer the suspense, though maybe a quick initial read, followed by a more careful return later would work best. The rest of this chapter is pretty straightforward with examples, but I think this chapter in particular should be re-read after getting farther, because it lays the foundations for the fundamental insights of Bayesian inference (at least, as I understand it so far).</p>

<p>On p. 62, section 4.2.2.1, he gets around to being very explicit about a point I had earlier regarding confusion between different kinds of probability.  He does finally point out that &theta;, i.e., the parameter that you actually care about testing with your data, is not always a probability, even though it has been in his examples of coin-flipping.  This is a point that I think an instructor could and should build in earlier, or at least watch out for, so that students don&#8217;t get the two completely confounded.</p>

<p>I like that Kruschke revisits the three goals of inference that he set up earlier, now clothing them in Bayesian terms.  This is excellent, though again, worthy of re-reading later.  I&#8217;ve heard noise about Bayes factors not being all that when it comes to model comparison, although that&#8217;s what Kruschke presents.  &#8220;I&#8217;ve heard noise&#8221; is about as good as I can say now, though, so that will be something to watch out for in the future, maybe.</p>

<p>I&#8217;d like to end this post by paraphrasing and trying to re-express a point that Kruschke makes towards the end of the chapter, and one that I think the whole first seven chapters or so are all building towards.  I&#8217;m hoping by paraphrasing, that I can solidify my own understanding.</p>

<p>The general point is to understand why complex computational methods (or at least complex to me) are needed for Bayesian analysis, when Bayes&#8217; Rule seems to be rather simple and elegant.  Coming at Bayesian analysis from the outside, I&#8217;ve had mixed impressions. One that it&#8217;s about a different philosophical take on how to draw inferences from data, which to me makes a good deal of sense.  The other is that there&#8217;s a lot of tricky computational and mathematical aspects of it, which makes it seem more esoteric or difficult or even problematic for practical use.  And I never understood how or why these things were connected. So my understanding from this chapter of Kruschke is that it all comes down to the &#8220;evidence,&#8221; to use the Bayesian term, that is, the denominator of Bayes&#8217; Rule.  To paraphrase Bayes&#8217; Rule in terms of a model parameter (I have a regression coefficient in mind, for example), what we care about is the probability of a model parameter being a particular value given some data (the posterior). This is the whole &#8220;updated belief given the data&#8221; idea.  But to get that, we need a prior belief, a likelihood, and evidence (again, all with the Bayesian sense of these words). The prior belief makes sense conceptually to me, and I can at least imagine how one might come up with such a thing. The likelihood is the probability of the data occurring given the model and parameters, and I can imagine that one can do that without too much difficulty, basically with a probability density.</p>

<p>So far, I have a pretty decent grip on this, I think.  From my NHST training, I&#8217;m comfortable thinking about data being the result of grabbing some numbers out of a bag.  What we are trying to understand is the properties of &#8220;the bag&#8221; (i.e., the world, the mind, etc.), but we can&#8217;t know everything in the bag, so we grab enough numbers to see if we can learn something about how the contents are structured.  The NHST way of doing things is to formulate a distribution of what the numbers would look like if the bag had no structure and everything was just random &#8212; the null hypothesis &#8212; and then we can say, with this boring imaginary bag, it would be very unlikely (like say, less that 5% probability) that we would get numbers like the ones we&#8217;ve drawn.  Therefore, the bag must be structured in some way.  The way I understand it, Bayesian likelihood is like saying &#8220;okay, imagine the bag has this property, what would random pulls from the bag look like,&#8221; and that&#8217;s the probability density of the data given that parameter.  While a little more involved than the NHST null hypothesis, because it&#8217;s not always just a &#8220;null hypothesis bag,&#8221; it&#8217;s not all that different, and I can imagine being able to come up with these values.</p>

<p>So the real trick, computationally, at least, seems to be the evidence. The evidence is the probability of the data happening across all possible parameter values. In the majority of cases I can think of that are of practical interest to a researcher, parameter values are pretty much always continuous, and therefore it&#8217;s impossible or impractical to actually calculate this value.  That&#8217;s where all the &#8220;fancy&#8221; computational and mathematical techniques like MCMC sampling come into play, as providing ways of estimating this quantity in a reasonable way.  And what makes it difficult is that because there&#8217;s a random element to this, sometimes the algorithms and computations can go astray, and that&#8217;s where a lot of active work is being done, to refine and improve the techniques for actually calculating the value of the evidence.</p>

<p>At least, this is how I&#8217;m seeing things now.  All you Bayesians out there, if I&#8217;m screwing it up, please enlighten me!</p>

<p>EDIT: I wrote the above the first time I went through the chapter, and I&#8217;m leaving it as-is, in case you&#8217;re reading the book along with me.  But I&#8217;m a little off base here. Hopefully I&#8217;ll get it sorted out better later.  As a sneak preview, there&#8217;s not really anything about the &#8220;evidence&#8221; that&#8217;s a problem, but it is an issue of efficient sampling from a large space, so I&#8217;m sorta partly right. But this is more or less confirming my general point from above: while this chapter is well-written and (at least to a complete novice) seems to articulate the core of Bayesian inference in an accessible way, my guess is that most people would benefit from re-visiting it, once they&#8217;ve gotten their hands dirtier with actually doing Bayesian analysis, in the next few chapters.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Scott Jackson</span></span>

      








  


<time datetime="2012-11-07T14:42:00-05:00" pubdate data-updated="true">Nov 7<span>th</span>, 2012</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/bayesian/'>Bayesian</a>, <a class='category' href='/blog/categories/kruschke/'>Kruschke</a>, <a class='category' href='/blog/categories/statistics/'>statistics</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4/" data-via="" data-counturl="http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4/" >Tweet</a>
  
  
  <div class="g-plusone" data-size="large"></div>
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2012/11/05/kruschke-chapter-3/" title="Previous Post: Kruschke Chapter 3">&laquo; Kruschke Chapter 3</a>
      
      
        <a class="basic-alignment right" href="/blog/2012/11/13/really-reproducible-research/" title="Next Post: Really reproducible research">Really reproducible research &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/11/17/kruschke-chapter-5/">Kruschke Chapter 5, Part 1</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/11/13/really-reproducible-research/">Really reproducible research</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/11/07/kruschke-chapter-4/">Kruschke Chapter 4</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/11/05/kruschke-chapter-3/">Kruschke Chapter 3</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/11/04/reproducibility-of-new-methods/">Reproducibility of new methods</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/shoestringpsycholing">@shoestringpsycholing</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'shoestringpsycholing',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>






  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Scott Jackson -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'shoestringpsycholing';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4/';
        var disqus_url = 'http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
