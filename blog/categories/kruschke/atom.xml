<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kruschke | Shoestring Psycholing]]></title>
  <link href="http://shoestringpsycholing.github.com/blog/categories/kruschke/atom.xml" rel="self"/>
  <link href="http://shoestringpsycholing.github.com/"/>
  <updated>2013-08-06T23:15:10-04:00</updated>
  <id>http://shoestringpsycholing.github.com/</id>
  <author>
    <name><![CDATA[Scott Jackson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 5, Part 3]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2013/02/16/kruschke-chapter-5-part-3/"/>
    <updated>2013-02-16T12:57:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2013/02/16/kruschke-chapter-5-part-3</id>
    <content type="html"><![CDATA[<p><em>Finally</em>, it's time for some actual examples of (very very simple) Bayesian analysis. The last part of the chapter gives a summary of "how to do Bayesian analysis," and Kruschke walks through the skeleton of steps that you need to do in order to use the code and concepts developed up to this point to analyze some (binary categorical) data. If you are working through this book yourself, I highly recommend the exercises.  There is a lot of pedagogical value there, so much that I think this chapter would be incomplete if you skipped them. In this post, I will walk through some analyses with some additional data, comparing the Bayesian results to a traditional analysis, and illustrating a few points I find interesting.</p>

<!--more-->


<p>Kruschke provides a lot of code, here and throughout the book. However, while these code examples work well to get through the examples and exercises, they are not necessarily well-suited to trying out other things. For example, the <code>BernBeta</code> function in the <code>BernBeta.R</code> script crams a whole lot of stuff into it. In addition to plotting the prior, likelihood, and posterior probabilities, it returns the shape parameters of the posterior. But it prints out some values on the plots (like the HDI information and the value of <em>p</em>(<em>D</em>)) which you can't access later.  So I tweaked the code of this function. I'll take you through the details in another post.  Here, I just want to note that the easiest way to replicate what I show in this post is to <code>clone</code> (or just download) my <a href="https://github.com/shoestringpsycholing/kruschke_remix">kruschke_remix</a> repo on GitHub and inspect/run the code yourself, which is contained in a <code>.R</code> script in the "post_code" folder.</p>

<p>The first thing is to load the functions and data we'll need.  Note that I'm trying to keep Kruschke's original code (which I stole directly from <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">his website</a>) in a separate folder in the repository from the "remix" code, which is first stolen, then tweaked. This is just to help keep track of which code is "original," and which has been messed with by me.</p>

<p>```
source("../code/Chapter5/bernbetaShPsy.R")
source("../kruschke_code/HDIofICDF.R")
std.err.perc &lt;- function(p, n) { sqrt(p*(1-p)/(n)) }</p>

<p>learn &lt;- read.delim("../data/LangLearnPilot.txt")
learn.sub &lt;- droplevels(learn[learn$Apt == "match.CaseB" &amp;</p>

<pre><code>                          learn$Test == 3 &amp;
                          learn$Condition == "CaseB", ])
</code></pre>

<p>```</p>

<p>The lines loading the data are picking out a specific subset of data, just for the sake of illustration, so don't worry about the details of what the data represents.  To simplify things, just assume that this data represents performance on a language test, and that each data point is an independent "correct" or "incorrect" (1 or 0) response, and since there are only two possible responses, guessing should get around 50% correct. And let's assume that the research question here is whether performance in this task is greater than chance or not. This is analagous to Kruschke's example of wanting to know whether this is a "fair coin" or not.</p>

<p>One simple traditional way to analyze the data is to compute a standard error for the overall proportion correct, and use a confidence interval of 1.96 times the standard error. That's what the next couple of lines of code represent (the function for computing the standard error was created in the block above).</p>

<p><code>
mu &lt;- mean(learn.sub$Score)
n &lt;- length(learn.sub$Score)
mu.ci &lt;- c(mu - 1.96 * std.err.perc(mu, n), mu + 1.96 * std.err.perc(mu, n))
</code></p>

<p>The overall accuracy is 41 correct out of 60 (a proportion of 0.683), and the 95% confidence interval goes from 0.566 to 0.801.  This basically means we can reject the hypothesis that the "true accuracy" is less than 0.566 or greater than 0.801 (56.6% or 80.1% accuracy), assuming the standard <em>p</em> &lt; .05 level of "significance."  Since we are interested in whether performance was better than chance (better than 0.50), then we can reject the "null" hypothesis that performance was at chance, since a value of 0.50 would be outside the range of 0.566 through 0.801.</p>

<p>So how does a Bayesian analysis differ?</p>

<p>In this chapter, Kruschke has simplified things considerably by picking the mathematical properties of the probability distributions very carefully. Since we are modeling our priors as a beta distribution and the likelihood as the Bernoulli likelihood function, the math works out very nicely, and computing the posterior boils down to just combining the prior and likelihood distributions in a straightforward way.  All of this math derives transparently from Bayes' Rule, and Kruschke goes through it step by step, so I won't reiterate the details here.</p>

<p>But the basic idea is that we use <a href="http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4/">Bayes' Rule</a> to compute the posterior distribution, and then find the Highest Density Interval (HDI) corresponding to 95% of the distribution. This has an analagous (though not exactly equivalent) interpretation as a confidence interval, because it's telling use that the most credible values for the parameter &theta; (which is the "accuracy" parameter we are curious about) fall within this range. So let's do the analysis, and see how it stacks up to the traditional confidence interval.</p>

<p>Let's start with a very simple prior, a shape of (1,1).  You can imagine that this is the prior that corresponds to recognizing that there are two possible answers, but you have no idea whatsoever about how responses are expected to pattern.  In other words, <em>any</em> value of accuracy is just as expected as any other. So by using this prior, we're going into the analysis expecting that performance could be 0% accurate, 50% accurate, 100% accurate, or anywhere in between, and all possibilites are equally likely (or unlikely).</p>

<p>The likelihood is a description of how likely the <em>data</em> is (i.e., how likely it is to get exactly 41 correct out of 60) for every possible value of the actual accuracy parameter.  So for example, even if you have a coin that is biased to flip heads, there's still a chance it could flip more tails than heads.  Or, if you have a coin that's perfectly "fair," the odds are not terribly high that it will flip <em>exactly</em> 50%-50% for any given number of tosses. In terms of our current data, even if the participants have an "at chance" ability to answer the test questions (i.e., they're just guessing randomly), there's still a chance they could get 0% correct, 50% correct, 100% correct, or anything in between.  So the likelihood says "okay, if we assume &theta; is 0.50, then the probability of getting 41 out of 60 is such-and-such, if we assume &theta; is 0.51, then the probability of getting 41 out ot 60 is so-and-so, etc. etc.," over the whole range of &theta; values from 0 to 1.</p>

<p>Finally, our posterior tells us, "okay, now that you've observed this data of 41 correct out of 60, what should the new distribution of belief about &theta; (i.e., accuracy) be?"  The following code uses my revised version of Kruschke's <code>BernBeta</code> function to compute the likelihood and posterior, then plots all three: prior, likelihood, and posterior densities, and annotates the plots with some details.  This is all mimicking Kruschke's original code (in a slightly different way). As an added comparison, I've plotted our traditional 95% confidence interval with red vertical lines superimposed on the posterior.</p>

<p><code>
bb.post1 &lt;- bern.beta(c(1, 1), learn.sub$Score)
plot(bb.post1)
abline(v = mu.ci, col = "red")
</code>
<img src="/images/bbpost1.jpg"></p>

<p>You can see that the values are <em>very</em> similar, though not identical, between the traditional 95% CI and the 95% HDI of the posterior (the red lines representing the CI are very close to the dotted lines representing the HDI). This should be fairly reassuring. And if we think about it, we should probably place a little more faith in the Bayesian HDI, because the CI is based on assumptions about asymptotic behavior, and we really only have 60 observations here.  Still, the Bayesian interval is <em>narrower</em> than the CI, so it's not the case that the CI is less conservative.</p>

<p>But what if we change the prior?  Let's try a prior with a shape of (30, 30).  This is similar to saying "we've observed 50%-50% behavior from an earlier set of 60 observations" as a prior, which is going to give a preference for belief in accuracy close to 0.50.  The following code does this, and gives us some new plots.</p>

<p><code>
bb.post2 &lt;- bern.beta(c(30, 30), learn.sub$Score)
plot(bb.post2)
abline(v = mu.ci, col = "red")
</code></p>

<p><img src="/images/bbpost2.jpg"></p>

<p>Okay, so let's stop and notice a few things.  First, we see how this new prior differs from the totally flat prior in the previous model, since it is now peaked over 0.50.  You can see that values above 0.6 or so, or below 0.4 or so (on the x axis) are pretty low, meaning we have little reason to expect such values. Next, the likelihood is unchanged. This is because it's still just the probability of the data (for each value of &theta;), and the data's still the same. Then note that this change in the prior had a pretty big impact on our posterior!  It's shifted way over, not at all aligned to the traditional 95% CI.  But it's also interesting that the HDI still goes from 0.504 to 0.679.  So if all we cared about was whether 0.50 was within the HDI, we'd come to the same overall conclusion, that performance is credibly better than chance.</p>

<p>But notice something else: the <em>actual</em> accuracy we observed was 0.683, and this is just <em>outside</em> the HDI of the second analysis!  In other words, given this prior belief we expressed in the second model, we should be pretty surprised to see the pattern of data that we actually collected. This is also captured by the difference in the evidences (remember "evidence" is a technical term that means "probability of data given the model").  This is smaller in the second model (though not <em>that</em> much smaller).</p>

<p>Now let's try a prior that's like saying "in a previous set of 60 observations, we saw 2/3rds accuracy."</p>

<p><code>
bb.post3 &lt;- bern.beta(c(40, 20), learn.sub$Score)
plot(bb.post3)
abline(v = mu.ci, col = "red")
</code></p>

<p><img src="/images/bbpost3.jpg"></p>

<p>Now you can see that the prior is in a similar place to the likelihood, and the result is a posterior that's narrower than what we saw in the first analysis.  This makes sense because it's like replicating an earlier set of data pretty closely. That is, the prior is reflecting a belief that we could get by observing 40 out of 60 correct in an earlier experiment, so getting another 41 out of 60 correct is expected to decrease our uncertainty about the exact accuracy level for the test. And decreases in uncertainty mean narrower posteriors, representing a narrower range of credible values. Also notice that now the evidence is more than 10x greater in this model than the previous model (which had a prior peaked over 0.50). Since the priors are the only things that differ between those two models, this is basically saying that this prior peaked over 0.67 or so "fits" the data better than a prior peaked over 0.50.</p>

<p>So now we have two different ways of articulating evidence for the belief that performance in our data is credibly higher than chance (higher that 0.50).  First, with a totally flat prior, which gives (perhaps unrealistically) equal likelihood that accuracy on the test could be 0% accurate, 100% accurate, or any value in between, our posterior HDI did <em>not</em> overlap 0.50, meaning that 0.50 is not a particularly believable value for accuracy. Thus we can conclude that performance is greater than chance. Second, we can compare a model that places highest belief in values close to 0.50 (close to chance) to a model that places highest belief around 0.67 (somewhat higher than chance), and see that the second model has a lot more support, given the data.  And thus we could argue that since the model with a prior higher than chance is better than a model around chance, it's more credible that accuracy is above chance.</p>

<p>Is there any difference to taking one of these approaches over the other to make an argument/inference? I'll be interested to see how these different approaches play out in more involved analyses.  But I think in this case, since the choice of a strong prior is fairly arbitrary, it feels simpler to stick with the flat prior (the first model above) and use the HDI to say whether or not performance is above chance (above 0.50). The fact that this lines up closely to a traditional CI derived from the standard error of proportions seems to fit, as well, given the goals of this analysis.</p>

<p>There are several more things I'd like to run through with this simple example, but I'll save those for some later posts.  If any of this discussion raises questions for you, please let me know in the comments!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 5, Part 2]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2013/02/14/kruschke-chapter-5-part-2/"/>
    <updated>2013-02-14T20:10:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2013/02/14/kruschke-chapter-5-part-2</id>
    <content type="html"><![CDATA[<p>So way back in <a href="http://shoestringpsycholing.github.com/blog/2012/11/17/kruschke-chapter-5/">November</a>, I promised the next post would elaborate a bit on model comparison. This part of the chapter, and the accompanying exercises, blew my mind a little.</p>

<!--more-->


<h2>Bayesian priors are part of the model</h2>

<p>I think one of the hardest things to get a handle on, coming from "traditional" NHST stats is the concept of priors.  Not just the concept, but how the heck you're supposed to <em>use</em> them.  Up to this point, Kruschke's presented them in very consistent, intuitive way.  Priors are a mathematical expression of your beliefs "prior" to the consideration of a particular set of data.  In the ongoing example in the book, this may be your belief about how fair a coin is before you start flipping.  In a more real research context, priors are often discussed (and Kruschke presents this idea, too) as characterizing "what we know already" before an experiment.</p>

<p>Extrapolating a bit, maybe you're doing a lexical decision priming experiment, where reaction times are usually in the 300-500 millisecond range, then you can be pretty sure you won't see priming effects of more than 100-200 milliseconds or so at the outer limit.  Maybe this is a lame example, but pick any real experiment or data set, and unless the experimental paradigm is completely new, you probably have some information from previous experiments about what generally you might find.</p>

<p>But I think where non-Baysians start to get nervous is when they realize that the posteriors (which seem like "results") depend in part on the priors.  So if you can make up the priors, can't you use that to manipulate your results?  And if priors represent prior "beliefs," isn't it hard to argue about what someone's beliefs are?  What if the experimenter's beliefs are different from my (the reader's) beliefs?  I think most of these worries are missing the point, but I'm still too novice to understand why, really.  But Kruschke's discussion of model comparison, and in particular the exercise 5.6, has given me a different way to think about it.  Not sure if I'm right yet, but it feels right.</p>

<p>The way I understand it is that priors aren't some squishy thing that are "off-limits" for discussion. They are part of the model of data being constructed, and therefore can be compared using model comparison. When I think of "model comparison" in a regression sense, what I think of is comparing different regression models (i.e., different sets of predictor variables), and checking fit statistics to see which model provides a better fit to the data.  Maybe a model with an interaction effect is better than one without it, so you take that as evidence that there is a "significant" interaction. In Bayesian model comparison, this might play out as comparing different sets of priors, because it's the <em>priors</em> where things are set like the range of possible (or likely) values that a parameter might take.  Back to the coin-flipping example, you could compare a model in which the prior is a strong belief in a fair coin to a model in which the prior is a weak belief in fairness (i.e., a wide spread in possible values for the fairness of the coin).</p>

<p>To put it another way, <strong>pitting different priors against each other looks like a way to explicitly test two competing hypotheses.</strong>  Maybe I'm missing something, but this is a totally different spin on priors than I had understood up to this point, and I think it makes sense.  If you want to stick to Kruschke's phrasing of "beliefs," then what you can do is compare different sets of beliefs.  There doesn't have to be one "most appropriate" set of beliefs before you analyze your data.</p>

<p>In the next post (on Kruschke's book), I'll try to finally look at some data and go through a very simple Bayesian analysis, parallel to what Kruschke does in the book.  Hopefully then some of these abstract discussion will start to make a little more sense.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 5, Part 1]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/17/kruschke-chapter-5/"/>
    <updated>2012-11-17T14:04:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/17/kruschke-chapter-5</id>
    <content type="html"><![CDATA[<p>So Chapter 5 is a big one.  I'm going to end up breaking it across several posts.  It's a big one for a few reasons. It's the first chapter in Part 2 of the book, representing the first "real" Bayesian analysis. In other words, we're finally getting to the point where we will actually apply Bayesian analysis to some data! In the interest of making these blog posts maximally useful for myself (and maybe some readers), I'm going to go through some additional data sets of my own, parallel to the examples Kruschke uses.</p>

<p>The other thing I'm going to try to do, which will end up stretching out these posts a bit, is to re-work some of the code that Kruschke supplies.  In a nutshell, I feel like Kruschke's code is probably aimed pretty well at students, who may want to be able to complete the exercises in the book, but who may or may not (a) know much about R, or (b) want to be able to apply the functions to more general situations (i.e., other data). I think in order for the code to be more useful to me personally, I'd like to re-work it a bit.  I've started a <a href="https://github.com/shoestringpsycholing/kruschke_remix">github repo for the Kruschke book here</a> (see also the links on the side of the blog).</p>

<p>In this post, I'll give an overview of the conceptual issues, and we'll get to the code and actual analysis in following posts.</p>

<!--more-->


<p>The point I have droned on about previously still holds in this chapter: the chapter is worth a re-read or two, especially after one gets a few more chapters into the book. In my initial read, it was a little hard to see the point of going through some of the mathematical derivations and equations, especially since I have a hunch that this kind of process is going to be quickly supplanted by the MCMC methods later. The best analogy I can think of that might resonate with people from linguistics or psycholinguistics is that it's a little like introducing some "old school" theoretical constructs in syntax or phonology, which will only be replaced by methods/theories later in the course. It's kind of a thin analogy, though, because the methods in this chapter of Kruschke are not invalid or outdated, but they do not seem to me to be representative of how Bayesian analysis is done in most actual cases for which you might want to do Bayesian analysis.</p>

<p>For example, he goes through a discussion about how the beta distribution is chosen based on its convenient mathematical properties. This seems totally irrelevant at first, since most of the time in actual analysis, you won't be stuck trying to find mathematically convenient ways of specifying priors in order to avoid MCMC, since you'll probably be doing MCMC anyway!</p>

<p>So what's the point? I think Kruschke has some very good pedagogical aims in mind, though in my first reading, I didn't think he made them clear enough. To me, the real reason to follow Kruschke through all the math is to be able to appreciate the points he makes in two short paragraphs on p. 84.  The first is:</p>

<blockquote><p>If the prior distribution is beta(&theta; | <em>a</em>, <em>b</em>) and the data have <em>z</em> heads in <em>N</em> flips, then the posterior distribution is beta(&theta; | <em>z</em> + <em>a</em>, <em>N</em> - <em>z</em> + <em>b</em>). The simplicity of that updating rule is one of the beauties of the mathematical approach to Bayesian inference.</p></blockquote>

<p>If you follow the math to this point, I think the importance of this really sinks in. Kruschke is trying to illustrate how straightforward and non-mysterious the process of going from prior (belief) to posterior (updated belief) is, when the math involved allows for straightforward computation. The same point is illustrated in the immediately following paragraphs about the prior and posterior means. I think one of the biggest "mysteries" to people just starting to learn about Bayesian data analysis (including myself) is the relationship between the "results" (i.e., the posterior) and the prior. I mean, I think we're used to thinking about the outcome of a statistical analysis as the results, and the data are just what you need in order to find out what the results are. In the NHST framework I was brought up in, the "hard" part is just making sure you're applying the right stats to give you the "correct" results. This Bayesian thing seems so much more squishy and amorphous because of the influence of the priors, or rather the lack of understanding about how the priors can influence the "results." But I think this is really misplacing what's difficult about Bayesian analysis, and I think this pedagogical move by Kruschke is a nice one, as an attempt to de-mystify the relationship between prior and posterior. When the math is simple, like it is in this case in Chapter 5, it's quite simple to see that the move from prior to posterior is extremely transparent, and if it didn't work like it does, it would seem completely wrong. Still, I think it took me at least two or three readings to appreciate this, and get beyond the annoyance of "why is Kruschke going through all this math, when this is a useless skill one we get to MCMC?"</p>

<p>Kruschke introduces the idea of a <em>region of practical equivalence</em> (ROPE) in this chapter, but like many other things, I think it raises more questions than it answers, and is best seen as just a first pass at the idea. He promises that Chapter 12 will work out a lot of these issues.</p>

<p>In reading the "Predicting Data" section (5.3.2, starting on p. 87), something occurred to me. Namely, that the process of predicting from prior data is absolutely opposite the intuitive <a href="http://en.wikipedia.org/wiki/Gambler%27s_fallacy">gambler's fallacy</a>. I could see this as a useful point of discussion in a class setting, especially if some of the students are still grappling with some basic ideas in statistics. The gambler's fallacy is this idea that previous data affects future data, so if you flip a coin and it turns up heads 10 times in a row, then somehow it's more likely to turn up tails on the next flip. This is an extremely appealing intuition (for reasons that I think are still up for debate in the cognitive science literature), and it's easy to fall victim to this kind of thinking, but it's simply false.</p>

<p>What's interesting to me is that not only is it false in Bayesian inference, but it's going the wrong way entirely. In Bayesian inference, you essentially start with some belief that the coin is fair, but if you then flip the coin and you get 10 heads in a row (and no tails), then your posterior would be updated essentially to believe that the coin was not quite fair, and it would increase the probability of you predicting another heads.  Of course, the degree to which you would predict another heads would depend on how strong your prior was. If you had a very strong belief that the coin was fair, it may only change a tiny amount. But still, the change is in the <em>opposite</em> direction of the gambler's fallacy. That is, if you see a run of heads, the "gambler" inside you may tell you that you should bet tails on the next flip, but a Bayesian would suggest that maybe the coin isn't fair after all. I just think this is an interesting difference between a common (but false) intuition about how probabilities work, and how Bayesian belief-updating works, and puts a different spin on this common "Stats 101" kind of example.</p>

<p>The section on model comparison raises a number of interesting issues, so I'll leave it to next time. All in all, this chapter is unfolding much like the previous ones: deceptively easy to follow on a first pass, but the more important pedagogical points may not sink in until another reading or two, and maybe then only after you get farther into the book and have time to absorb things more. The downside of this is that you might have trouble seeing the point to some of the more detailed mathematical excursions. (And by the way "detailed math" is relative: the math is really quite straightforward, but to many typical social scientists, it will seem like a lot in places, speaking as one myself.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 4]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4/"/>
    <updated>2012-11-07T14:42:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4</id>
    <content type="html"><![CDATA[<p>Woohoo! Bayes' Rule! Or should it be Bayes's?  Kruschke goes with Bayes', so I guess I will, too, but the linguist in me really things it ought to be Bayes's, or at least pronounced that way.  I'd certainly say Jonas's rule, if I knew a guy name Jonas with a rule.  Ok, sorry, back to Bayes and Kruschke.</p>

<p>I really like the initial examples he starts off with.  For whatever reason, equations are a lot of hard work for me, even though I like them, and it's hard for me to understand them deeply enough to have an intuitive feel for what they are saying.  The rain/clouds example is way more accessible, and the playing card probabilities are a nice enough "toy" example that actual numbers can be calculated, so overall I think he's done a great job in choosing examples.</p>

<p>The gist of Bayes' Rule is that it sets up a relationship between conditional probabilities, allowing you to calculate something you want to know, from quantities that you already have (or can estimate).  Back to the idea of beliefs as probabilities, Bayesian inference boils down to the idea of calculating beliefs (probability of some parameter) given data, which is a conditional probability, like calculating the probability of rain (a parameter) given clouds (data).  And the point and magic of Bayes' Rule is that this can be calculated as a function of other probabilities, which we can get easier access to directly.</p>

<!--more-->


<p>Of course, "easier" is relative, and that's where all the computational stuff starts coming into play.  But in this chapter, Kruschke sticks to the coin-flipping and playing-card examples, because the probabilities involved there can be calculated pretty easily.</p>

<p>But again <a href="http://shoestringpsycholing.github.com/blog/2012/11/05/kruschke-chapter-3/">like in the previous chapter</a>, I feel like sometimes I needed to suspend my thoughts for a little to follow how Kruschke lays out things, because it took me a while to get the connection between what he starts off talking about as examples, and things that I see as valuable.  I'm coming to this book from a fair amount of experience with a bunch of NHST tools, even "fancy" ones like mixed-effects models, and there have been times in these initial chapters where I was having trouble seeing the relevance to what I wanted to do, which was try to understand how Bayesian stats are an alternative to the NHST way of doing things.  When he finally gets to talking about belief in a model, or belief in a model parameter, I could start recognizing those things as goals, but it was hard to follow the lead-up.  So again --- and I'm starting to sound like a broken record --- depending on the audience, a bit of a back-and-forth re-reading of some of these initial chapters will probably be beneficial, in order to connect his very nice, clear description of concepts to the more complex things that you will inevitably care more about. To put it a little more negatively, at this point in the book, if you're coming in with any expertise in other statistical methods, you may feel impatient, and it may be hard to work through these chapters without a clear view of how they connect to what you actually want to do. My two cents is that it's worth the effort to suffer the suspense, though maybe a quick initial read, followed by a more careful return later would work best. The rest of this chapter is pretty straightforward with examples, but I think this chapter in particular should be re-read after getting farther, because it lays the foundations for the fundamental insights of Bayesian inference (at least, as I understand it so far).</p>

<p>On p. 62, section 4.2.2.1, he gets around to being very explicit about a point I had earlier regarding confusion between different kinds of probability.  He does finally point out that &theta;, i.e., the parameter that you actually care about testing with your data, is not always a probability, even though it has been in his examples of coin-flipping.  This is a point that I think an instructor could and should build in earlier, or at least watch out for, so that students don't get the two completely confounded.</p>

<p>I like that Kruschke revisits the three goals of inference that he set up earlier, now clothing them in Bayesian terms.  This is excellent, though again, worthy of re-reading later.  I've heard noise about Bayes factors not being all that when it comes to model comparison, although that's what Kruschke presents.  "I've heard noise" is about as good as I can say now, though, so that will be something to watch out for in the future, maybe.</p>

<p>I'd like to end this post by paraphrasing and trying to re-express a point that Kruschke makes towards the end of the chapter, and one that I think the whole first seven chapters or so are all building towards.  I'm hoping by paraphrasing, that I can solidify my own understanding.</p>

<p>The general point is to understand why complex computational methods (or at least complex to me) are needed for Bayesian analysis, when Bayes' Rule seems to be rather simple and elegant.  Coming at Bayesian analysis from the outside, I've had mixed impressions. One that it's about a different philosophical take on how to draw inferences from data, which to me makes a good deal of sense.  The other is that there's a lot of tricky computational and mathematical aspects of it, which makes it seem more esoteric or difficult or even problematic for practical use.  And I never understood how or why these things were connected. So my understanding from this chapter of Kruschke is that it all comes down to the "evidence," to use the Bayesian term, that is, the denominator of Bayes' Rule.  To paraphrase Bayes' Rule in terms of a model parameter (I have a regression coefficient in mind, for example), what we care about is the probability of a model parameter being a particular value given some data (the posterior). This is the whole "updated belief given the data" idea.  But to get that, we need a prior belief, a likelihood, and evidence (again, all with the Bayesian sense of these words). The prior belief makes sense conceptually to me, and I can at least imagine how one might come up with such a thing. The likelihood is the probability of the data occurring given the model and parameters, and I can imagine that one can do that without too much difficulty, basically with a probability density.</p>

<p>So far, I have a pretty decent grip on this, I think.  From my NHST training, I'm comfortable thinking about data being the result of grabbing some numbers out of a bag.  What we are trying to understand is the properties of "the bag" (i.e., the world, the mind, etc.), but we can't know everything in the bag, so we grab enough numbers to see if we can learn something about how the contents are structured.  The NHST way of doing things is to formulate a distribution of what the numbers would look like if the bag had no structure and everything was just random --- the null hypothesis --- and then we can say, with this boring imaginary bag, it would be very unlikely (like say, less that 5% probability) that we would get numbers like the ones we've drawn.  Therefore, the bag must be structured in some way.  The way I understand it, Bayesian likelihood is like saying "okay, imagine the bag has this property, what would random pulls from the bag look like," and that's the probability density of the data given that parameter.  While a little more involved than the NHST null hypothesis, because it's not always just a "null hypothesis bag," it's not all that different, and I can imagine being able to come up with these values.</p>

<p>So the real trick, computationally, at least, seems to be the evidence. The evidence is the probability of the data happening across all possible parameter values. In the majority of cases I can think of that are of practical interest to a researcher, parameter values are pretty much always continuous, and therefore it's impossible or impractical to actually calculate this value.  That's where all the "fancy" computational and mathematical techniques like MCMC sampling come into play, as providing ways of estimating this quantity in a reasonable way.  And what makes it difficult is that because there's a random element to this, sometimes the algorithms and computations can go astray, and that's where a lot of active work is being done, to refine and improve the techniques for actually calculating the value of the evidence.</p>

<p>At least, this is how I'm seeing things now.  All you Bayesians out there, if I'm screwing it up, please enlighten me!</p>

<p>EDIT: I wrote the above the first time I went through the chapter, and I'm leaving it as-is, in case you're reading the book along with me.  But I'm a little off base here. Hopefully I'll get it sorted out better later.  As a sneak preview, there's not really anything about the "evidence" that's a problem, but it is an issue of efficient sampling from a large space, so I'm sorta partly right. But this is more or less confirming my general point from above: while this chapter is well-written and (at least to a complete novice) seems to articulate the core of Bayesian inference in an accessible way, my guess is that most people would benefit from re-visiting it, once they've gotten their hands dirtier with actually doing Bayesian analysis, in the next few chapters.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 3]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/05/kruschke-chapter-3/"/>
    <updated>2012-11-05T22:26:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/05/kruschke-chapter-3</id>
    <content type="html"><![CDATA[<p>This is the first really substantial chapter; the first two were really just a warm-up. Kruschke presents this chapter as a kind of intro to probability, but (naturally) he makes some very specific choices about what to approach and how to approach it.  A lot of it comes across as kind of foreshadowing, which makes it a little hard to follow or grasp the point of at times. Ultimately, I think this chapter should go pretty quick the first read-through, to prime some ideas, but the ideas probably won't really start to sink in until they get applied later.</p>

<!--more-->


<p>Another overall comment: Kruschke points this out at some point himself, but the fact is that throughout this chapter, he's really talking about two kinds (or contexts) of probability. One is as a mathematical expression of belief, which, I assume, is pretty much a given for any kind of Bayesian analysis.  The other is the parameter (&theta;) of interest that one is trying to estimate.  I mean, in all the coin-flipping examples, there is the probability of heads, which is just a parameter of the data-generating process.  In a different kind of data or analysis, it might be something else, not probability (I'm guessing that when we get to Bayesian regression, the parameter will be something like a regression coefficient, which is not a probability). And then there's the probability (or belief) in a parameter value (<em>p</em>(&theta;)), which is always going to be expressed as probability (e.g., for regression, there should be probabilities/beliefs for different coefficient values).  As intuitive and appealing as the coin-flipping examples are, I find this double-sided discussion of probability to be kind of confusing.</p>

<p>So, on to the details of the chapter. Kruschke focuses on the simple case of predicting coin flips and deciding whether the coin is fair.  This has the advantage of not only being conceptually intuitive, but also numerically convenient, allowing for a variety of analytic approaches that don't require the heavier-duty computational tools of MCMC, and I'm sure this was a choice Kruschke made by design.</p>

<p>But illustrating my quibble above, he starts with a discussion about how the fairness of the coin (the parameter &theta;) is a probability, and this can be simulated as a long-run relative frequency, or derived mathematically, and then he switches abruptly to a brief discussion about probability as a expression of subjective belief, even discussing how one might heuristically estimate one's own internal numerical belief in something. I find this back-and-forth discussion a little disorienting, and paradoxically, it feels like he goes into too much detail and not enough detail. The discussion of "calibrating" one's subjective belief seems to invite more questions and confusions than are necessary to make the simple point that in Bayesian analysis, "belief" is represented mathematically by probabilities. But it's not quite in-depth enough to foster a deep understanding or exploration of the ideas.</p>

<p>I think if I were using this for a course, I would want to <em>either</em> (a) spend more time on fleshing out the ideas in this chapter, and take care to very explicitly pull apart the concepts of probability-as-parameter vs. probability-as-belief, or (b) speed through or defer sections of the chapter entirely, jump straight into some analysis, and refer back to sections in this chapter to help present the basic ideas behind the analysis.</p>

<p>Working through the rest of the chapter, I feel like he's foreshadowing the steps from Chapters 5 through 7, from a purely analytic approach, to a grid approximation, to MCMC sampling.  He touches on a number of interesting points, including some nice discussions of:</p>

<ul>
<li>probability distributions (<em>mass</em> for discrete outcomes and <em>density</em> for continuous)</li>
<li>the normal probability distribution</li>
<li>the relationships between mean, variance, standard deviations, and medians</li>
<li>Highest Density Intervals (which, in my NHST conceptual background, are like Bayesian confidence intervals)</li>
<li>two-way probabilities, marginal probabilities, and conditional probabilities. This is where he starts to lead towards the crux of Bayes's rule.</li>
</ul>


<p>But all these things feel like sort of incomplete foreshadowing of what's to come.  Priming the pump, as it were, but I personally have a hard time retaining stuff when I don't see the point of it until later.  So again, I think in practice I would recommend zipping through and then revisiting this chapter once you've gotten into the applications later.</p>

<p>The other main way I can think to describe Kruschke's writing so far is that it is a very uneven flow of information.  There are stretches of very informal, easy-to-read text that glide by, and then I hit a sentence or two that quickly assert or review some equation or other point that is very dense, and I feel like I have to read it over several times and work through it in my head to make sure I'm understanding.  This is not really a criticism, just a description of my personal experience in reading.</p>

<p>These relatively minor worries and hand-wringings aside, I think this is a nice, concise, readable chapter.  I think he does a good job of discussing some basic concepts in probability in an extremely approachable way, in order to set up a better understanding of what's to come. It's certainly not an exhaustive treatment, but it's useful enough that I'd recommend a good reading of it, even for peole coming to the book with a decent stats background. But I really do think that most of the good points may be a little lost or unappreciated until some of the later topics in the book are broached.</p>
]]></content>
  </entry>
  
</feed>
