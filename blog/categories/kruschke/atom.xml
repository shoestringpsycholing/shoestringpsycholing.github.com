<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kruschke | Shoestring Psycholing]]></title>
  <link href="http://shoestringpsycholing.github.com/blog/categories/kruschke/atom.xml" rel="self"/>
  <link href="http://shoestringpsycholing.github.com/"/>
  <updated>2012-11-05T22:57:11-05:00</updated>
  <id>http://shoestringpsycholing.github.com/</id>
  <author>
    <name><![CDATA[Scott Jackson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 3]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/05/kruschke-chapter-3/"/>
    <updated>2012-11-05T22:26:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/05/kruschke-chapter-3</id>
    <content type="html"><![CDATA[<p>This is the first really substantial chapter; the first two were really just a warm-up. Kruschke presents this chapter as a kind of intro to probability, but (naturally) he makes some very specific choices about what to approach and how to approach it.  A lot of it comes across as kind of foreshadowing, which makes it a little hard to follow or grasp the point of at times. Ultimately, I think this chapter should go pretty quick the first read-through, to prime some ideas, but the ideas probably won't really start to sink in until they get applied later.</p>

<p>Another overall comment: Kruschke points this out at some point himself, but the fact is that throughout this chapter, he's really talking about two kinds (or contexts) of probability. One is as a mathematical expression of belief, which, I assume, is pretty much a given for any kind of Bayesian analysis.  The other is the parameter (&theta;) of interest that one is trying to estimate.  I mean, in all the coin-flipping examples, there is the probability of heads, which is just a parameter of the data-generating process.  In a different kind of data or analysis, it might be something else, not probability (I'm guessing that when we get to Bayesian regression, the parameter will be something like a regression coefficient, which is not a probability). And then there's the probability (or belief) in a parameter value (<em>p</em>(&theta;)), which is always going to be expressed as probability (e.g., for regression, there should be probabilities/beliefs for different coefficient values).  As intuitive and appealing as the coin-flipping examples are, I find this double-sided discussion of probability to be kind of confusing.</p>

<p>So, on to the details of the chapter. Kruschke focuses on the simple case of predicting coin flips and deciding whether the coin is fair.  This has the advantage of not only being conceptually intuitive, but also numerically convenient, allowing for a variety of analytic approaches that don't require the heavier-duty computational tools of MCMC, and I'm sure this was a choice Kruschke made by design.</p>

<p>But illustrating my quibble above, he starts with a discussion about how the fairness of the coin (the parameter &theta;) is a probability, and this can be simulated as a long-run relative frequency, or derived mathematically, and then he switches abruptly to a brief discussion about probability as a expression of subjective belief, even discussing how one might heuristically estimate one's own internal numerical belief in something. I find this back-and-forth discussion a little disorienting, and paradoxically, it feels like he goes into too much detail and not enough detail. The discussion of "calibrating" one's subjective belief seems to invite more questions and confusions than are necessary to make the simple point that in Bayesian analysis, "belief" is represented mathematically by probabilities. But it's not quite in-depth enough to foster a deep understanding or exploration of the ideas.</p>

<p>I think if I were using this for a course, I would want to <em>either</em> (a) spend more time on fleshing out the ideas in this chapter, and take care to very explicitly pull apart the concepts of probability-as-parameter vs. probability-as-belief, or (b) speed through or defer sections of the chapter entirely, jump straight into some analysis, and refer back to sections in this chapter to help present the basic ideas behind the analysis.</p>

<p>Working through the rest of the chapter, I feel like he's foreshadowing the steps from Chapters 5 through 7, from a purely analytic approach, to a grid approximation, to MCMC sampling.  He touches on a number of interesting points, including some nice discussions of:</p>

<ul>
<li>probability distributions (<em>mass</em> for discrete outcomes and <em>density</em> for continuous)</li>
<li>the normal probability distribution</li>
<li>the relationships between mean, variance, standard deviations, and medians</li>
<li>Highest Density Intervals (which, in my NHST conceptual background, are like Bayesian confidence intervals)</li>
<li>two-way probabilities, marginal probabilities, and conditional probabilities. This is where he starts to lead towards the crux of Bayes's rule.</li>
</ul>


<p>But all these things feel like sort of incomplete foreshadowing of what's to come.  Priming the pump, as it were, but I personally have a hard time retaining stuff when I don't see the point of it until later.  So again, I think in practice I would recommend zipping through and then revisiting this chapter once you've gotten into the applications later.</p>

<p>The other main way I can think to describe Kruschke's writing so far is that it is a very uneven flow of information.  There are stretches of very informal, easy-to-read text that glide by, and then I hit a sentence or two that quickly assert or review some equation or other point that is very dense, and I feel like I have to read it over several times and work through it in my head to make sure I'm understanding.  This is not really a criticism, just a description of my personal experience in reading.</p>

<p>These relatively minor worries and hand-wringings aside, I think this is a nice, concise, readable chapter.  I think he does a good job of discussing some basic concepts in probability in an extremely approachable way, in order to set up a better understanding of what's to come. It's certainly not an exhaustive treatment, but it's useful enough that I'd recommend a good reading of it, even for peole coming to the book with a decent stats background. But I really do think that most of the good points may be a little lost or unappreciated until some of the later topics in the book are broached.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 2]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/03/kruschke-chapter-2/"/>
    <updated>2012-11-03T13:48:00-04:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/03/kruschke-chapter-2</id>
    <content type="html"><![CDATA[<p>(<a href="http://shoestringpsycholing.github.com/blog/2012/11/02/naive-bayesian">Here's a link to the intro and Chapter 1 discussion</a>)</p>

<p>In this chapter, Kruschke does a nice job of succinctly laying out the primary goals of statistical inferencing, and gives a pretty clear, intuitive description of what prior and posterior beliefs are about. To paraphrase in a nutshell, prior beliefs represent our beliefs (including level of uncertainty in those beliefs) before data collection or observation, and posterior beliefs reflect what we believe after taking data into account.</p>

<p>This chapter strikes me as a nice place to do some discussions in a class about what the goals of statistics are, the idea of updating beliefs via statistics, etc.  For example, I can easily see some people being uneasy about the idea of mathematical formulas determining beliefs, in a philosophical sense. Kruschke does touch on this briefly, mentioning basically that beliefs that cannot be influenced by data are essentially "out of bounds," but he does so in kind of a flippant way. I think even if we are talking about scientific beliefs, the idea of an analysis telling you what you <em>should</em> believe (and Kruschke does use the word "should") can be uncomfortable, and not entirely realistic.</p>

<p>To clarify, in case you're new to Bayesian inference, one of the concepts behind Bayesian inference is that it gives a clear mathematical formulation for how beliefs should be changed (there's that "should" again), given the prior beliefs and the data. For example, a detective might be able to express her beliefs about suspects in terms of relative probabilities. So maybe if a woman was murdered, there could be a relatively high prior belief that it was the jealous ex-husband, and relatively low prior belief that it was the woman's 2-year-old daughter. If the detective finds the daughter's fingerprints on the murder weapon, it may only slightly affect her (nearly zero) belief that the daughter did it, where finding the husband's fingerprints may make a big difference. The point is that Bayesian inference gives a mathematical framework for actually computing how beliefs should be updated. But given what people often mean when they talk about "beliefs," the idea of a mathematical system telling you how you should change your beliefs given some data might be uncomfortable.</p>

<p>Of course, once you get a little farther, the Bayesian concept of "belief" is pretty circumscribed and clear, and it doesn't quite mean what most people mean by "belief," but I could see this point in the book being a good place for some discussion along these lines in a class. Especially because it's such a different kind of approach compared to the null-hypothesis significance testing (NHST) way of thinking about things.  If NHST is new to you, too, the NHST way of thinking about things is more that you approach a set of data and ask "what are the odds that this data occurred this way purely by chance?"  And if you decide that the odds are sufficiently low (by convention in many fields, less than 5%, or 1-in-20 odds), then you draw the conclusion that the "null hypothesis" (the hypothesis that whatever you're looking at happened just by chance) is probably false. This comes up more directly later in the book, but that's the gist, if all this stats stuff is new to you.</p>

<p>But back to the three goals as Kruschke presents them: (1) estimation of parameter values, (2) prediction of data values, and (3) model comparison.  Depending on the context of who's reading this book, I could see the benefit of some more examples of these things, but the presentation in the text is nice and clear.  Kruschke makes an interesting point about Bayesian analysis "intrinsically adjust[ing] for model complexity" (p. 14), but the significance of this is likely to be lost on anyone not familiar with model comparison in other (e.g., NHST) methods. In terms of the audience of the book, I think these points will be much more interesting and meaningful to anyone who's actually done data analysis before, and they may fly right by someone who's relatively new to it.</p>

<p>Finally, Kruschke includes a short intro to the <a href="http://www.r-project.org/">R software and programming language</a>. There's not really a great way to introduce R in 7 pages, but Kruschke does a decent job of giving the minimum needed to help people get the ball rolling.  Again, here's another spot where his <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">website</a> outpaces the book already, because the website makes the excellent recommendation of using <a href="http://www.rstudio.org/">RStudio</a>, instead of the (IMHO) inferior  <a href="http://www.sciviews.org/Tinn-R/">Tinn-R</a>. No knock against the fine folks who put together Tinn-R.  This is just one of those things that makes it hard to write static books about R.  RStudio is a pretty new player in the GUI game, but it's quickly become the favorite for general users for a lot of good reasons.  In any case, Kruschke will not make you an expert at R. But he gives you enough to run through the code in the book, cookbook-style. I suspect for anyone needing to work with real data, some additional resources or expertise in R will be needed to go from raw data to Bayesian analysis. <strong>R pet peeve alert:</strong> throughout the book, Kruschke uses the style of using <code>=</code> instead of <code>&lt;-</code> for assignment. This may help many people coming from other programming languages, but it's a little non-idiomatic for R.  But this is a pretty inconsequential pet peeve, and since the book is about Bayesian analysis, not R coding style, I think overall this is not a bad choice.</p>

<p>In summary, it's a good chapter and covers a bunch of introductory bases.  Depending on who's reading the book, and what the goals are (class textbook vs. self-study), a good amount of supplementary material or discussion could be useful, or it could be fine to just skim this chapter to get on to the meatier stuff.</p>
]]></content>
  </entry>
  
</feed>
