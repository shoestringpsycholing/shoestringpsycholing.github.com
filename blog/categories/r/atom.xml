<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: R | Shoestring Psycholing]]></title>
  <link href="http://shoestringpsycholing.github.com/blog/categories/r/atom.xml" rel="self"/>
  <link href="http://shoestringpsycholing.github.com/"/>
  <updated>2013-08-06T23:15:10-04:00</updated>
  <id>http://shoestringpsycholing.github.com/</id>
  <author>
    <name><![CDATA[Scott Jackson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 5, Part 3]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2013/02/16/kruschke-chapter-5-part-3/"/>
    <updated>2013-02-16T12:57:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2013/02/16/kruschke-chapter-5-part-3</id>
    <content type="html"><![CDATA[<p><em>Finally</em>, it's time for some actual examples of (very very simple) Bayesian analysis. The last part of the chapter gives a summary of "how to do Bayesian analysis," and Kruschke walks through the skeleton of steps that you need to do in order to use the code and concepts developed up to this point to analyze some (binary categorical) data. If you are working through this book yourself, I highly recommend the exercises.  There is a lot of pedagogical value there, so much that I think this chapter would be incomplete if you skipped them. In this post, I will walk through some analyses with some additional data, comparing the Bayesian results to a traditional analysis, and illustrating a few points I find interesting.</p>

<!--more-->


<p>Kruschke provides a lot of code, here and throughout the book. However, while these code examples work well to get through the examples and exercises, they are not necessarily well-suited to trying out other things. For example, the <code>BernBeta</code> function in the <code>BernBeta.R</code> script crams a whole lot of stuff into it. In addition to plotting the prior, likelihood, and posterior probabilities, it returns the shape parameters of the posterior. But it prints out some values on the plots (like the HDI information and the value of <em>p</em>(<em>D</em>)) which you can't access later.  So I tweaked the code of this function. I'll take you through the details in another post.  Here, I just want to note that the easiest way to replicate what I show in this post is to <code>clone</code> (or just download) my <a href="https://github.com/shoestringpsycholing/kruschke_remix">kruschke_remix</a> repo on GitHub and inspect/run the code yourself, which is contained in a <code>.R</code> script in the "post_code" folder.</p>

<p>The first thing is to load the functions and data we'll need.  Note that I'm trying to keep Kruschke's original code (which I stole directly from <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">his website</a>) in a separate folder in the repository from the "remix" code, which is first stolen, then tweaked. This is just to help keep track of which code is "original," and which has been messed with by me.</p>

<p>```
source("../code/Chapter5/bernbetaShPsy.R")
source("../kruschke_code/HDIofICDF.R")
std.err.perc &lt;- function(p, n) { sqrt(p*(1-p)/(n)) }</p>

<p>learn &lt;- read.delim("../data/LangLearnPilot.txt")
learn.sub &lt;- droplevels(learn[learn$Apt == "match.CaseB" &amp;</p>

<pre><code>                          learn$Test == 3 &amp;
                          learn$Condition == "CaseB", ])
</code></pre>

<p>```</p>

<p>The lines loading the data are picking out a specific subset of data, just for the sake of illustration, so don't worry about the details of what the data represents.  To simplify things, just assume that this data represents performance on a language test, and that each data point is an independent "correct" or "incorrect" (1 or 0) response, and since there are only two possible responses, guessing should get around 50% correct. And let's assume that the research question here is whether performance in this task is greater than chance or not. This is analagous to Kruschke's example of wanting to know whether this is a "fair coin" or not.</p>

<p>One simple traditional way to analyze the data is to compute a standard error for the overall proportion correct, and use a confidence interval of 1.96 times the standard error. That's what the next couple of lines of code represent (the function for computing the standard error was created in the block above).</p>

<p><code>
mu &lt;- mean(learn.sub$Score)
n &lt;- length(learn.sub$Score)
mu.ci &lt;- c(mu - 1.96 * std.err.perc(mu, n), mu + 1.96 * std.err.perc(mu, n))
</code></p>

<p>The overall accuracy is 41 correct out of 60 (a proportion of 0.683), and the 95% confidence interval goes from 0.566 to 0.801.  This basically means we can reject the hypothesis that the "true accuracy" is less than 0.566 or greater than 0.801 (56.6% or 80.1% accuracy), assuming the standard <em>p</em> &lt; .05 level of "significance."  Since we are interested in whether performance was better than chance (better than 0.50), then we can reject the "null" hypothesis that performance was at chance, since a value of 0.50 would be outside the range of 0.566 through 0.801.</p>

<p>So how does a Bayesian analysis differ?</p>

<p>In this chapter, Kruschke has simplified things considerably by picking the mathematical properties of the probability distributions very carefully. Since we are modeling our priors as a beta distribution and the likelihood as the Bernoulli likelihood function, the math works out very nicely, and computing the posterior boils down to just combining the prior and likelihood distributions in a straightforward way.  All of this math derives transparently from Bayes' Rule, and Kruschke goes through it step by step, so I won't reiterate the details here.</p>

<p>But the basic idea is that we use <a href="http://shoestringpsycholing.github.com/blog/2012/11/07/kruschke-chapter-4/">Bayes' Rule</a> to compute the posterior distribution, and then find the Highest Density Interval (HDI) corresponding to 95% of the distribution. This has an analagous (though not exactly equivalent) interpretation as a confidence interval, because it's telling use that the most credible values for the parameter &theta; (which is the "accuracy" parameter we are curious about) fall within this range. So let's do the analysis, and see how it stacks up to the traditional confidence interval.</p>

<p>Let's start with a very simple prior, a shape of (1,1).  You can imagine that this is the prior that corresponds to recognizing that there are two possible answers, but you have no idea whatsoever about how responses are expected to pattern.  In other words, <em>any</em> value of accuracy is just as expected as any other. So by using this prior, we're going into the analysis expecting that performance could be 0% accurate, 50% accurate, 100% accurate, or anywhere in between, and all possibilites are equally likely (or unlikely).</p>

<p>The likelihood is a description of how likely the <em>data</em> is (i.e., how likely it is to get exactly 41 correct out of 60) for every possible value of the actual accuracy parameter.  So for example, even if you have a coin that is biased to flip heads, there's still a chance it could flip more tails than heads.  Or, if you have a coin that's perfectly "fair," the odds are not terribly high that it will flip <em>exactly</em> 50%-50% for any given number of tosses. In terms of our current data, even if the participants have an "at chance" ability to answer the test questions (i.e., they're just guessing randomly), there's still a chance they could get 0% correct, 50% correct, 100% correct, or anything in between.  So the likelihood says "okay, if we assume &theta; is 0.50, then the probability of getting 41 out of 60 is such-and-such, if we assume &theta; is 0.51, then the probability of getting 41 out ot 60 is so-and-so, etc. etc.," over the whole range of &theta; values from 0 to 1.</p>

<p>Finally, our posterior tells us, "okay, now that you've observed this data of 41 correct out of 60, what should the new distribution of belief about &theta; (i.e., accuracy) be?"  The following code uses my revised version of Kruschke's <code>BernBeta</code> function to compute the likelihood and posterior, then plots all three: prior, likelihood, and posterior densities, and annotates the plots with some details.  This is all mimicking Kruschke's original code (in a slightly different way). As an added comparison, I've plotted our traditional 95% confidence interval with red vertical lines superimposed on the posterior.</p>

<p><code>
bb.post1 &lt;- bern.beta(c(1, 1), learn.sub$Score)
plot(bb.post1)
abline(v = mu.ci, col = "red")
</code>
<img src="/images/bbpost1.jpg"></p>

<p>You can see that the values are <em>very</em> similar, though not identical, between the traditional 95% CI and the 95% HDI of the posterior (the red lines representing the CI are very close to the dotted lines representing the HDI). This should be fairly reassuring. And if we think about it, we should probably place a little more faith in the Bayesian HDI, because the CI is based on assumptions about asymptotic behavior, and we really only have 60 observations here.  Still, the Bayesian interval is <em>narrower</em> than the CI, so it's not the case that the CI is less conservative.</p>

<p>But what if we change the prior?  Let's try a prior with a shape of (30, 30).  This is similar to saying "we've observed 50%-50% behavior from an earlier set of 60 observations" as a prior, which is going to give a preference for belief in accuracy close to 0.50.  The following code does this, and gives us some new plots.</p>

<p><code>
bb.post2 &lt;- bern.beta(c(30, 30), learn.sub$Score)
plot(bb.post2)
abline(v = mu.ci, col = "red")
</code></p>

<p><img src="/images/bbpost2.jpg"></p>

<p>Okay, so let's stop and notice a few things.  First, we see how this new prior differs from the totally flat prior in the previous model, since it is now peaked over 0.50.  You can see that values above 0.6 or so, or below 0.4 or so (on the x axis) are pretty low, meaning we have little reason to expect such values. Next, the likelihood is unchanged. This is because it's still just the probability of the data (for each value of &theta;), and the data's still the same. Then note that this change in the prior had a pretty big impact on our posterior!  It's shifted way over, not at all aligned to the traditional 95% CI.  But it's also interesting that the HDI still goes from 0.504 to 0.679.  So if all we cared about was whether 0.50 was within the HDI, we'd come to the same overall conclusion, that performance is credibly better than chance.</p>

<p>But notice something else: the <em>actual</em> accuracy we observed was 0.683, and this is just <em>outside</em> the HDI of the second analysis!  In other words, given this prior belief we expressed in the second model, we should be pretty surprised to see the pattern of data that we actually collected. This is also captured by the difference in the evidences (remember "evidence" is a technical term that means "probability of data given the model").  This is smaller in the second model (though not <em>that</em> much smaller).</p>

<p>Now let's try a prior that's like saying "in a previous set of 60 observations, we saw 2/3rds accuracy."</p>

<p><code>
bb.post3 &lt;- bern.beta(c(40, 20), learn.sub$Score)
plot(bb.post3)
abline(v = mu.ci, col = "red")
</code></p>

<p><img src="/images/bbpost3.jpg"></p>

<p>Now you can see that the prior is in a similar place to the likelihood, and the result is a posterior that's narrower than what we saw in the first analysis.  This makes sense because it's like replicating an earlier set of data pretty closely. That is, the prior is reflecting a belief that we could get by observing 40 out of 60 correct in an earlier experiment, so getting another 41 out of 60 correct is expected to decrease our uncertainty about the exact accuracy level for the test. And decreases in uncertainty mean narrower posteriors, representing a narrower range of credible values. Also notice that now the evidence is more than 10x greater in this model than the previous model (which had a prior peaked over 0.50). Since the priors are the only things that differ between those two models, this is basically saying that this prior peaked over 0.67 or so "fits" the data better than a prior peaked over 0.50.</p>

<p>So now we have two different ways of articulating evidence for the belief that performance in our data is credibly higher than chance (higher that 0.50).  First, with a totally flat prior, which gives (perhaps unrealistically) equal likelihood that accuracy on the test could be 0% accurate, 100% accurate, or any value in between, our posterior HDI did <em>not</em> overlap 0.50, meaning that 0.50 is not a particularly believable value for accuracy. Thus we can conclude that performance is greater than chance. Second, we can compare a model that places highest belief in values close to 0.50 (close to chance) to a model that places highest belief around 0.67 (somewhat higher than chance), and see that the second model has a lot more support, given the data.  And thus we could argue that since the model with a prior higher than chance is better than a model around chance, it's more credible that accuracy is above chance.</p>

<p>Is there any difference to taking one of these approaches over the other to make an argument/inference? I'll be interested to see how these different approaches play out in more involved analyses.  But I think in this case, since the choice of a strong prior is fairly arbitrary, it feels simpler to stick with the flat prior (the first model above) and use the HDI to say whether or not performance is above chance (above 0.50). The fact that this lines up closely to a traditional CI derived from the standard error of proportions seems to fit, as well, given the goals of this analysis.</p>

<p>There are several more things I'd like to run through with this simple example, but I'll save those for some later posts.  If any of this discussion raises questions for you, please let me know in the comments!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Back with some tips]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2013/02/13/back-with-some-tips/"/>
    <updated>2013-02-13T20:40:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2013/02/13/back-with-some-tips</id>
    <content type="html"><![CDATA[<p>Finally getting back to the blog after a long hiatus. I got a little hung up with putting together a good post for the next segment in the Kruschke/Bayesian saga, and I've been too busy with other things to have the time to put some polish on it.  So I've decided instead to try out the other route and just post frequently, without worrying too much about polish.  We'll see how it goes.  I will pick back up with Kruschke, but I want to prioritize posting <em>some</em>thing frequently, instead of getting too perfectionist about it.</p>

<p>One of the continuing themes I intend to post on is the use of various software tools, with a special focus on tools for <a href="http://shoestringpsycholing.github.com/blog/2012/11/13/really-reproducible-research/">reproducible research</a>.  So I'll finish off today's post with three random tips about two of my favorite pieces of software.</p>

<!--more-->


<h2>Eliminate duplicate rows in your data in R</h2>

<p><a href="http://www.r-project.org/">R</a> is great.  I will post a lot about it. I use it a <em>lot</em>. But there's still so much for me to learn.  One incredibly handy thing I didn't realize was that it's ridiculously easy to take a messy data set and eliminate duplicate rows.  So for example, imagine that you're reading in a bunch of data, and for some reason, some rows are duplicated.  Maybe you merged several data sets at some other point in your processing, or maybe there is some redundancy or mistake in the data you're reading in.  Well, good news, you can eliminate all those annoying duplicates (which would otherwise mess up your analysis with one simple line:</p>

<pre><code>my.dataset &lt;- unique(my.dataset)
</code></pre>

<p>That's it!  You're welcome.  I have no idea how it took me over five years to learn this.</p>

<h2>Drag and drop files into Emacs</h2>

<p>Yes, I'm an <a href="http://www.gnu.org/software/emacs/">Emacs</a> user.  Still a total novice, but it's now the place I do virtually all my work, unless someone forces me to deal with a manuscript in Word.  It's a complete work environment, and I love it more all the time.  Nothing's perfect, [insert gratutious Emacs joke here involving neckbeards or "Emacs pinky" or it being a decent operating system without a good text editor, etc.], but it's hugely improved my productivity, and has made it possible for me to even contemplate being able to manage a workflow supporting reproducible research.</p>

<p>But like I said, I'm still a complete novice.  I <em>just</em> learned that you can drag-and-drop files into Emacs to open them.  Like, you have Emacs open, and you're looking through your files in a folder (yes, Emacs can browse file directories really well, too, but sometimes I need to be able to look at bunches of files in my GUI file system), and you can just grab a file from the folder and drag it over to Emacs and <em>pow</em>, it's open.</p>

<p>This just killed off the last remaining use I had for other text editors.  I have a copy of <a href="http://notepad-plus-plus.org/">Notepad++</a>, which is very good, don't get me wrong, and I have an older copy of <a href="http://www.textpad.com/">TextPad</a>, which served me well when I was just getting started with Perl back in grad school, and sometimes when I'm rummaging through files and just need to peek at a text file quickly, I just right-click to open it in one of these other editors.  It was always just a few clicks or keystrokes too long to open it in Emacs to just peek.  But now that I can just drag it into Emacs, when I virtually laways have Emacs running already?  It's over! This won't replace the normal way to open files while working in Emacs for me, but it will replace the final use case I had for any other text editor.</p>

<h2>Editing text from <code>dired</code> in Emacs</h2>

<p>One more bit of Emacs happiness.  One of the reasons you get sucked into doing <em>everything</em> in Emacs is that you can interact with <em>everything</em> as if it were editable text.  Today was another example of how this has paid off for me.  I had to go through literally <em>thousands</em> of files from an old experiment to see if I was missing any, and if so which ones. It also dawned on me that I might have to do this multiple times.  Ugh.  But then I realized that I could just use Emacs' <code>dired</code> functionality to display the folder contents (i.e., all the file names) in Emacs.  I could then copy and paste (or in Emacs parlance, "kill" and "yank") those filenames into a simple text file, and then use the great <a href="http://vimeo.com/1168225">rectangle editing in <code>cua-mode</code></a> to quickly delete all the stuff in front of the file names, and finally I could easily read these names into <code>R</code> and get a summary of exactly which subjects were missing exactly which items (based on the way the files were named).  Just lovely, and yes, I did have to repeat this a few more times, and it took zero effort to do so, and was about 1000x faster and a lot more accurate than doing it by hand.  I'm sure I could have done this in a bunch of different ways, but the ability to quickly pull up the directory contents in Emacs and treat it as Just Text really made my day.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 5, Part 1]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/17/kruschke-chapter-5/"/>
    <updated>2012-11-17T14:04:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/17/kruschke-chapter-5</id>
    <content type="html"><![CDATA[<p>So Chapter 5 is a big one.  I'm going to end up breaking it across several posts.  It's a big one for a few reasons. It's the first chapter in Part 2 of the book, representing the first "real" Bayesian analysis. In other words, we're finally getting to the point where we will actually apply Bayesian analysis to some data! In the interest of making these blog posts maximally useful for myself (and maybe some readers), I'm going to go through some additional data sets of my own, parallel to the examples Kruschke uses.</p>

<p>The other thing I'm going to try to do, which will end up stretching out these posts a bit, is to re-work some of the code that Kruschke supplies.  In a nutshell, I feel like Kruschke's code is probably aimed pretty well at students, who may want to be able to complete the exercises in the book, but who may or may not (a) know much about R, or (b) want to be able to apply the functions to more general situations (i.e., other data). I think in order for the code to be more useful to me personally, I'd like to re-work it a bit.  I've started a <a href="https://github.com/shoestringpsycholing/kruschke_remix">github repo for the Kruschke book here</a> (see also the links on the side of the blog).</p>

<p>In this post, I'll give an overview of the conceptual issues, and we'll get to the code and actual analysis in following posts.</p>

<!--more-->


<p>The point I have droned on about previously still holds in this chapter: the chapter is worth a re-read or two, especially after one gets a few more chapters into the book. In my initial read, it was a little hard to see the point of going through some of the mathematical derivations and equations, especially since I have a hunch that this kind of process is going to be quickly supplanted by the MCMC methods later. The best analogy I can think of that might resonate with people from linguistics or psycholinguistics is that it's a little like introducing some "old school" theoretical constructs in syntax or phonology, which will only be replaced by methods/theories later in the course. It's kind of a thin analogy, though, because the methods in this chapter of Kruschke are not invalid or outdated, but they do not seem to me to be representative of how Bayesian analysis is done in most actual cases for which you might want to do Bayesian analysis.</p>

<p>For example, he goes through a discussion about how the beta distribution is chosen based on its convenient mathematical properties. This seems totally irrelevant at first, since most of the time in actual analysis, you won't be stuck trying to find mathematically convenient ways of specifying priors in order to avoid MCMC, since you'll probably be doing MCMC anyway!</p>

<p>So what's the point? I think Kruschke has some very good pedagogical aims in mind, though in my first reading, I didn't think he made them clear enough. To me, the real reason to follow Kruschke through all the math is to be able to appreciate the points he makes in two short paragraphs on p. 84.  The first is:</p>

<blockquote><p>If the prior distribution is beta(&theta; | <em>a</em>, <em>b</em>) and the data have <em>z</em> heads in <em>N</em> flips, then the posterior distribution is beta(&theta; | <em>z</em> + <em>a</em>, <em>N</em> - <em>z</em> + <em>b</em>). The simplicity of that updating rule is one of the beauties of the mathematical approach to Bayesian inference.</p></blockquote>

<p>If you follow the math to this point, I think the importance of this really sinks in. Kruschke is trying to illustrate how straightforward and non-mysterious the process of going from prior (belief) to posterior (updated belief) is, when the math involved allows for straightforward computation. The same point is illustrated in the immediately following paragraphs about the prior and posterior means. I think one of the biggest "mysteries" to people just starting to learn about Bayesian data analysis (including myself) is the relationship between the "results" (i.e., the posterior) and the prior. I mean, I think we're used to thinking about the outcome of a statistical analysis as the results, and the data are just what you need in order to find out what the results are. In the NHST framework I was brought up in, the "hard" part is just making sure you're applying the right stats to give you the "correct" results. This Bayesian thing seems so much more squishy and amorphous because of the influence of the priors, or rather the lack of understanding about how the priors can influence the "results." But I think this is really misplacing what's difficult about Bayesian analysis, and I think this pedagogical move by Kruschke is a nice one, as an attempt to de-mystify the relationship between prior and posterior. When the math is simple, like it is in this case in Chapter 5, it's quite simple to see that the move from prior to posterior is extremely transparent, and if it didn't work like it does, it would seem completely wrong. Still, I think it took me at least two or three readings to appreciate this, and get beyond the annoyance of "why is Kruschke going through all this math, when this is a useless skill one we get to MCMC?"</p>

<p>Kruschke introduces the idea of a <em>region of practical equivalence</em> (ROPE) in this chapter, but like many other things, I think it raises more questions than it answers, and is best seen as just a first pass at the idea. He promises that Chapter 12 will work out a lot of these issues.</p>

<p>In reading the "Predicting Data" section (5.3.2, starting on p. 87), something occurred to me. Namely, that the process of predicting from prior data is absolutely opposite the intuitive <a href="http://en.wikipedia.org/wiki/Gambler%27s_fallacy">gambler's fallacy</a>. I could see this as a useful point of discussion in a class setting, especially if some of the students are still grappling with some basic ideas in statistics. The gambler's fallacy is this idea that previous data affects future data, so if you flip a coin and it turns up heads 10 times in a row, then somehow it's more likely to turn up tails on the next flip. This is an extremely appealing intuition (for reasons that I think are still up for debate in the cognitive science literature), and it's easy to fall victim to this kind of thinking, but it's simply false.</p>

<p>What's interesting to me is that not only is it false in Bayesian inference, but it's going the wrong way entirely. In Bayesian inference, you essentially start with some belief that the coin is fair, but if you then flip the coin and you get 10 heads in a row (and no tails), then your posterior would be updated essentially to believe that the coin was not quite fair, and it would increase the probability of you predicting another heads.  Of course, the degree to which you would predict another heads would depend on how strong your prior was. If you had a very strong belief that the coin was fair, it may only change a tiny amount. But still, the change is in the <em>opposite</em> direction of the gambler's fallacy. That is, if you see a run of heads, the "gambler" inside you may tell you that you should bet tails on the next flip, but a Bayesian would suggest that maybe the coin isn't fair after all. I just think this is an interesting difference between a common (but false) intuition about how probabilities work, and how Bayesian belief-updating works, and puts a different spin on this common "Stats 101" kind of example.</p>

<p>The section on model comparison raises a number of interesting issues, so I'll leave it to next time. All in all, this chapter is unfolding much like the previous ones: deceptively easy to follow on a first pass, but the more important pedagogical points may not sink in until another reading or two, and maybe then only after you get farther into the book and have time to absorb things more. The downside of this is that you might have trouble seeing the point to some of the more detailed mathematical excursions. (And by the way "detailed math" is relative: the math is really quite straightforward, but to many typical social scientists, it will seem like a lot in places, speaking as one myself.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kruschke Chapter 2]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/03/kruschke-chapter-2/"/>
    <updated>2012-11-03T13:48:00-04:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/03/kruschke-chapter-2</id>
    <content type="html"><![CDATA[<p>(<a href="http://shoestringpsycholing.github.com/blog/2012/11/02/naive-bayesian">Here's a link to the intro and Chapter 1 discussion</a>)</p>

<p>In this chapter, Kruschke does a nice job of succinctly laying out the primary goals of statistical inferencing, and gives a pretty clear, intuitive description of what prior and posterior beliefs are about. To paraphrase in a nutshell, prior beliefs represent our beliefs (including level of uncertainty in those beliefs) before data collection or observation, and posterior beliefs reflect what we believe after taking data into account.</p>

<!--more-->


<p>This chapter strikes me as a nice place to do some discussions in a class about what the goals of statistics are, the idea of updating beliefs via statistics, etc.  For example, I can easily see some people being uneasy about the idea of mathematical formulas determining beliefs, in a philosophical sense. Kruschke does touch on this briefly, mentioning basically that beliefs that cannot be influenced by data are essentially "out of bounds," but he does so in kind of a flippant way. I think even if we are talking about scientific beliefs, the idea of an analysis telling you what you <em>should</em> believe (and Kruschke does use the word "should") can be uncomfortable, and not entirely realistic.</p>

<p>To clarify, in case you're new to Bayesian inference, one of the concepts behind Bayesian inference is that it gives a clear mathematical formulation for how beliefs should be changed (there's that "should" again), given the prior beliefs and the data. For example, a detective might be able to express her beliefs about suspects in terms of relative probabilities. So maybe if a woman was murdered, there could be a relatively high prior belief that it was the jealous ex-husband, and relatively low prior belief that it was the woman's 2-year-old daughter. If the detective finds the daughter's fingerprints on the murder weapon, it may only slightly affect her (nearly zero) belief that the daughter did it, where finding the husband's fingerprints may make a big difference. The point is that Bayesian inference gives a mathematical framework for actually computing how beliefs should be updated. But given what people often mean when they talk about "beliefs," the idea of a mathematical system telling you how you should change your beliefs given some data might be uncomfortable.</p>

<p>Of course, once you get a little farther, the Bayesian concept of "belief" is pretty circumscribed and clear, and it doesn't quite mean what most people mean by "belief," but I could see this point in the book being a good place for some discussion along these lines in a class. Especially because it's such a different kind of approach compared to the null-hypothesis significance testing (NHST) way of thinking about things.  If NHST is new to you, too, the NHST way of thinking about things is more that you approach a set of data and ask "what are the odds that this data occurred this way purely by chance?"  And if you decide that the odds are sufficiently low (by convention in many fields, less than 5%, or 1-in-20 odds), then you draw the conclusion that the "null hypothesis" (the hypothesis that whatever you're looking at happened just by chance) is probably false. This comes up more directly later in the book, but that's the gist, if all this stats stuff is new to you.</p>

<p>But back to the three goals as Kruschke presents them: (1) estimation of parameter values, (2) prediction of data values, and (3) model comparison.  Depending on the context of who's reading this book, I could see the benefit of some more examples of these things, but the presentation in the text is nice and clear.  Kruschke makes an interesting point about Bayesian analysis "intrinsically adjust[ing] for model complexity" (p. 14), but the significance of this is likely to be lost on anyone not familiar with model comparison in other (e.g., NHST) methods. In terms of the audience of the book, I think these points will be much more interesting and meaningful to anyone who's actually done data analysis before, and they may fly right by someone who's relatively new to it.</p>

<p>Finally, Kruschke includes a short intro to the <a href="http://www.r-project.org/">R software and programming language</a>. There's not really a great way to introduce R in 7 pages, but Kruschke does a decent job of giving the minimum needed to help people get the ball rolling.  Again, here's another spot where his <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">website</a> outpaces the book already, because the website makes the excellent recommendation of using <a href="http://www.rstudio.org/">RStudio</a>, instead of the (IMHO) inferior  <a href="http://www.sciviews.org/Tinn-R/">Tinn-R</a>. No knock against the fine folks who put together Tinn-R.  This is just one of those things that makes it hard to write static books about R.  RStudio is a pretty new player in the GUI game, but it's quickly become the favorite for general users for a lot of good reasons.  In any case, Kruschke will not make you an expert at R. But he gives you enough to run through the code in the book, cookbook-style. I suspect for anyone needing to work with real data, some additional resources or expertise in R will be needed to go from raw data to Bayesian analysis. <strong>R pet peeve alert:</strong> throughout the book, Kruschke uses the style of using <code>=</code> instead of <code>&lt;-</code> for assignment. This may help many people coming from other programming languages, but it's a little non-idiomatic for R.  But this is a pretty inconsequential pet peeve, and since the book is about Bayesian analysis, not R coding style, I think overall this is not a bad choice.</p>

<p>In summary, it's a good chapter and covers a bunch of introductory bases.  Depending on who's reading the book, and what the goals are (class textbook vs. self-study), a good amount of supplementary material or discussion could be useful, or it could be fine to just skim this chapter to get on to the meatier stuff.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Naive Bayesian]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/02/naive-bayesian/"/>
    <updated>2012-11-02T07:04:00-04:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/02/naive-bayesian</id>
    <content type="html"><![CDATA[<h2>Huh?</h2>

<p>A big part of my day-to-day involves data analysis, usually statistical data analysis. The stats scene has been in some flux lately, at least in the fields I work in, and interestingly, I think a lot of that has resulted, directly or indirectly, from the availability of the <a href="http://www.r-project.org/">R statistical language/package</a>. Long story short, it has increased accessibility to methods that are not all that new in the scheme of things, but which have not been terribly accessible.  More on all that another day.</p>

<p>Today, the topic is <a href="http://en.wikipedia.org/wiki/Thomas_Bayes">Bayesian</a> stats. The word "Bayesian" is a bit of a buzz-word for a wide range of different topics in computational and statistical work, but despite this, it's a really important modern approach to data and analysis.  What exactly is it, and what's the big deal?  Damned if I know, but in an ongoing series of posts, I intend to find out a lot more, and share my thoughts here.</p>

<!--more-->


<p>So here's the idea:  starting virtually from scratch, I'm going to walk through a recent <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">book</a> by <a href="http://www.indiana.edu/~kruschke/">John Kruschke</a>, and give my own knee-jerk reactions, as a kind of extended review of the book, and the usefulness of Bayesian methods for language scientists (and maybe others). Kruschke himself is a very um... <em>outspoken</em> advocate of Bayesian analysis (see this <a href="http://www.indiana.edu/~kruschke/AnOpenLetter.htm">"Open Letter"</a>), and while I'm generally pretty sympathetic and find most of these arguments pretty convincing, I'll try to take a skeptical stance when going through the book.</p>

<p>My own stats background is pretty hodge-podgy, but I can hold my own in comparison to most folks in my field. My hope is that by going through the book in this way, I'll get more out of the book myself, and it might help other folks decide whether looking into Bayesian stats could be worth their time, too.  Big props go to Amber Springer for getting a local reading group started on this book. I wasn't able to attend as much as I'd have liked to, but going to a couple of meetings really got the ball rolling for me, and I appreciate it!</p>

<h2>The Big Deal, in a nutshell</h2>

<p>If all this is utterly mysterious and the word "Bayesian" might as well be "Martian," here's the basic gist of why I'm spending time on this. "Traditional" stats in the fields I work in are more or less descended from seminal work by <a href="http://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a> and others, and can also be referred to as "Null Hypothesis Significance Testing." If you've ever looked for a "p-value" to tell you if your experiment "worked," this is the framework you're used to. It boils down to a system of making inferences based on data, because that's ultimately what we want, to look at data and have it tell us something so we can make inferential conclusions.  Like looking at language learning outcome measures and using those to tell you whether Teaching Method A was more effective than Teaching Method B. Or whatever.</p>

<p>The point made by folks like Kruschke is quite simply that the NHST system of making inferences is broken, and we need to switch to Bayesian methods, which involves a different system of making inferences from data. There has been quite the war within the field of statistics, with both sides (NHST and Bayesian) attacking the other program as obviously wrong and utterly misguided. My impression is that this polemic has faded a little, and other folks, like <a href="http://andrewgelman.com/">Andrew Gelman</a>, are not quite as divisive, but they often claim that Bayesian methods have distinct advantages over NHST methods. Wanting to understand what those advantages might be, and whether they could help with the sometimes very sticky data analysis situations I find myself in, are my main motivations.</p>

<p>That, and trying to understand what <a href="http://www.nhsilbert.net/">Noah Silbert</a> is going on about half the time.</p>

<p>So without further ado, I'm just going to launch into each chapter, throwing out my reactions and thoughts.</p>

<h2>Chapter 1</h2>

<p>This is a very short chapter, but a nice one to have.  It walks you through the structure of the book, and tries to assuage your fears that this book might be too much for you. In terms of prerequisites, Kruschke says that you don't really need any programming experience, and as long as you have a "dim knowledge of basic calculus," you should be fine.  I'm pretty sure my R skills will be up to snuff, but I will be a good test for his claims about calculus.  My knowledge (specifically, my memory of calculus from my high school course um... 18(!) years ago) is about as dim as it gets.  This chapter also gets you an intro to Kruschke's writing style, which is pretty goofy.  I mean that in a good way, though.  It's very conversational, and peppered with plenty of corny humor.  I imagine Kruschke's students groaning a lot (maybe on the inside, if they're respectful) in class, but staying awake because of it, and I suspect this will play out similarly in the book. I think this is probably a good thing, but we'll see.</p>

<p>One note, I didn't do an exhaustive search, but Kruschke gives HTTP addresses for both the Elsevier site and his personal site, and it looks like his personal site is where you should go to get programs, etc.  It's a little more scattered, visually, but his site is much more complete, and looks like it has a bunch of updated versions of programs, exercises, etc., where the Elsevier site has the more "basic" version of the code.</p>
]]></content>
  </entry>
  
</feed>
