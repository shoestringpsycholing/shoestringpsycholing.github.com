<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: reproducibility | Shoestring Psycholing]]></title>
  <link href="http://shoestringpsycholing.github.com/blog/categories/reproducibility/atom.xml" rel="self"/>
  <link href="http://shoestringpsycholing.github.com/"/>
  <updated>2013-02-15T09:11:52-05:00</updated>
  <id>http://shoestringpsycholing.github.com/</id>
  <author>
    <name><![CDATA[Scott Jackson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Really reproducible research]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/13/really-reproducible-research/"/>
    <updated>2012-11-13T17:46:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/13/really-reproducible-research</id>
    <content type="html"><![CDATA[<p>I think a <em>lot</em> about the related notions of <em>reproducibility</em> and <em>replicability</em> in research. I intend for these to be major reoccurring themes. Today I saw a big special issue of <a href="http://pps.sagepub.com/"><em>Perspectives on Psychological Science</em></a> on the topic of reproducibility, so I thought it would be good to break up the Bayesian stuff with a short rant about reproducibility.</p>

<!--more-->


<p>First, I'd like to propose a distinction between replicability and reproducibility. Replicability is about <em>results</em> and reproducibility is about <em>methods</em>. These are not independent, though, more like two sides of the same coin. If your results are replicable, that means that someone else can basically find the same pattern. This is a basic, fundamental goal of science. If someone finds that standing on your head while practicing a foreign language leads to better retention of new vocabulary, great. But if no one else can produce that result, then it calls the initial result into question.</p>

<p>This is a pretty big issue these days. The special issue I mention above is just one of the many signs. There's been a lot of recent discussion about how the publishing system in the social sciences (psychology has been getting the most attention, but I think this is true in virtually every field I'm acquainted with) actually discourages replication. Sensational results end up getting into more "important" journals, and from a purely statistical point of view, sensational results really ought to be rare things. The result is over-publication of results that may not hold up well when people try to replicate them. And then there's the problem that there's no viable venue to published "failed" studies, which do not show any effects. I'll make separate posts for each of these topics at some point.  The point here is that those of us who like to call ourselves "scientists" should hold ourselves to a better standard of replicability that the current publishing system encourages. Otherwise, we are just flooding the market with results that no one can really trust, and how can one formulate effective or accurate theories when trying to explain false results?</p>

<p>But how do you get replication?  That's where reproducible methods come in.  What if I run an experiment, get some interesting results, and then someone from a competing theoretical point of view says they tried to replicate, but can't, therefore casting doubt on my results?  Having truly <em>reproducible</em> methods is my safeguard. This means that my methods are transparent enough that other people can perform the experiment in a similar enough way, or that if someone tries to replicate and can't, they can compare their methods to mine, to see what might be different. There are some current standards that people try to keep in how they report their methods, to facilitate this, but I think we're still really a long way from true reproducibility. The key morpheme here is the <em>-able</em> part of <em>reproducible</em>. That is, if it's extremely difficult to reproduce methods, that is less reproducible. It can often be difficult to reproduce even your own methods, because there are so many small decision points along the way, and there is often considerable work at each step.</p>

<p>The good news is that there has been a recent rise in tools to make reproducible research easier, and to improve on the <em>-able</em> part, especially in reproducing statistical analysis. But I believe that we need to extend beyond reproducible stats. Sharing your data and your <a href="http://www.r-project.org/">R code</a> that you used for your analysis is a great first step, but that's all it is: a first step.</p>

<p>So ultimately, improving reproducible methods is one of the most important parts of addressing the replication problem. Even if we "fix" the publishing system so that failed replications are more visible, without better means to understand (1) how to replicate someone's work and (2) what potential differences between studies could be explaining replication failures, we'll still be in the dark when it comes to actually evaluating reproducibility.</p>

<p>If this is an interesting issue to you, stay tuned to this blog, as I plan to work through some of the concepts involved, as well as some of the practical means to actually achieve better reproducibility.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Reproducibility of new methods]]></title>
    <link href="http://shoestringpsycholing.github.com/blog/2012/11/04/reproducibility-of-new-methods/"/>
    <updated>2012-11-04T19:56:00-05:00</updated>
    <id>http://shoestringpsycholing.github.com/blog/2012/11/04/reproducibility-of-new-methods</id>
    <content type="html"><![CDATA[<p>One of the main themes I'd like to talk about in this blog is the notion of <em>reproducibility</em> in research. I'll leave more in-depth discussion of the fundamental ideas for other posts; today I just want to comment on something that occurred to me while working on a paper re-submission, and I think it's also really relevant to the ongoing Bayesian stats posts as well. The issue is: <strong>new methods are inherently less reproducible</strong>.  Let me explain.</p>

<p><a href="http://www.youtube.com/watch?v=GZYhDMCOyww">No, there is too much. Let me sum up.</a></p>

<!--more-->


<p>These days, mixed-effects models are rapidly overtaking the traditional ANOVA in some parts of the field. If the <a href="http://www.indiana.edu/~kruschke/AnOpenLetter.htm">Bayesians have their way</a> (see also <a href="http://shoestringpsycholing.github.com/blog/2012/11/02/naive-bayesian/">this post</a>), then Bayesian data analysis may make a similar sweep through my fields in another few years. But what happens when you write a paper using these shiny new methods, and either (a) your reviewers or (b) much of the audience for the paper (or both) don't understand the methods? Maybe you even include the <a href="http://www.r-project.org/">R code</a> for your analysis.  On the one hand, your analysis could be easily reproduced by anyone in the world, since R can be run for free on virtually any computer platform.  On the other hand, though, other people may find it impossible to <em>really</em> reproduce your analysis, since they don't understand it.</p>

<p>The problem is that this can cause trust to break down. Maybe the reviewers/readers decide to trust to because you seem smart and seem to know what you're doing. But in my experience, we are not trained to give people the benefit of the doubt, especially when reviewing articles for publication. The main question seems to be "why are they using this 'fancy' new method, when my old method (e.g., ANOVA) seems to work just fine?"  And even if an answer for why the new method is superior can be supplied, if it's not adequately explained, then it can't really be replicated or fully understood.</p>

<p>So what makes a method <em>really</em> reproducible?  Not just the means, but the understanding. Just as I don't expect a typical middle-schooler or even college student to be able to <em>really</em> replicate or reproduce most published analyses or results in psycholinguistics (even though they may be technically capable of re-running someone's R code), I don't expect that most psycholinguists (including myself at present) could reproduce a complex Bayesian analysis.</p>

<p>I'll leave the subtleties of this for another day, but I think it's an interesting and important issue, which I have not seen discussed explicitly, but of which I have seen (and felt!) very concrete examples.  So here's a thought: what if the future of truly reproducible research is not just "including code and data," but also including tutorials or other means to <em>educate</em> readers and reviewers? How far could or should this be pushed?</p>
]]></content>
  </entry>
  
</feed>
